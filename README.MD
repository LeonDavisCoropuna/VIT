
# VTCNN: Visual Transformer CNN

Este proyecto implementa una arquitectura hÃ­brida que combina **Convoluciones (CNN)** con **Transformers** para el procesamiento de imÃ¡genes, especÃ­ficamente sobre el dataset **Fashion-MNIST**. La arquitectura principal se denomina `VTCNN` y se compone de:

- Bloque CNN inicial para extracciÃ³n de caracterÃ­sticas espaciales locales.
- Una o mÃ¡s capas Visual Transformer (`VisualTransformer`) que operan sobre embeddings.
- Capas densas para clasificaciÃ³n.

---

## ğŸ§  Arquitectura General

```

Input Image
â†“
Conv2DLayer (conv\_layer)
â†“
BatchNorm2DLayer (bn)
â†“
\[reshape + transpose]
â†“
VisualTransformer (1 o mÃ¡s capas)
â†“
reshape â†’ CNN format
â†“
MaxPool2D
â†“
Flatten
â†“
Dense
â†“
Softmax

```

---

## ğŸ” DescripciÃ³n de Componentes

### ğŸ”¹ CNN (Embedding Espacial)

La imagen de entrada es pasada por:

1. **Conv2DLayer**: extrae patrones locales (filtros 3x3 con padding).
2. **BatchNorm2DLayer**: estabiliza y normaliza la salida.
3. **Flatten + Transpose**: convierte el mapa 4D `[N, C, H, W]` a una secuencia `[N, HW, C]` para poder alimentar al Transformer.

Este proceso actÃºa como un **encoder convolucional** que genera un embedding espacial por pÃ­xel (o patch), antes de pasar a la parte atencional.

---

### ğŸ”¸ VisualTransformer

Este bloque introduce atenciÃ³n basada en **transformers**, pero con entradas visuales.

**Componentes internos:**

- `FilterTokenizer`: genera una secuencia de **tokens** desde el mapa de caracterÃ­sticas de la imagen.
- `TransformerLayer`: aplica atenciÃ³n multi-cabeza sobre estos tokens, modelando relaciones a largo alcance.
- `ProjectorLayer`: fusiona los tokens y el mapa original si `is_projected = true`.

El flujo en un `VisualTransformer` es:

```

[feature map] â†’ Tokenizer â†’ Transformer â†’ [opcional] Projector â†’ salida

````

MÃºltiples bloques `VisualTransformer` pueden apilarse. El primer bloque recibe Ãºnicamente el mapa convolucional. Los siguientes bloques reciben tanto el mapa como los tokens anteriores.

---

### ğŸ”¹ MaxPool + Dense

Luego del procesamiento por Transformer, se reconstruye la estructura de imagen y se aplica:

- `MaxPool2DLayer`: reduce espacialmente (ej: de 28x28 a 14x14).
- `FlattenLayer`: aplana para pasar a capa densa.
- `DenseLayer`: capa de salida con `num_classes` neuronas.
- `SoftmaxLayer`: produce las probabilidades de clasificaciÃ³n.

---

## âš™ï¸ Entrenamiento

En `main.cpp`, el modelo es entrenado con el optimizador `SGD` y `cross-entropy loss`:

```cpp
trainer.train(/*epochs=*/50, /*log_every=*/100, device);
````

Se usa `Fashion-MNIST`, cargado con `load_dataset`.

---

## ğŸ§ª Backward Propagation

El `backward` se implementa de forma personalizada y recorre en orden inverso todos los componentes, permitiendo entrenamiento extremo a extremo.

Para los Transformers, la retropropagaciÃ³n incluye:

* `projector->backward()`
* `transformer->backward()`
* `tokenizer->backward()`

Con manejo explÃ­cito de los gradientes para tokens y mapa de caracterÃ­sticas.

---

## ğŸ§  Â¿Por quÃ© usar Visual Transformers?

Este enfoque combina:

* La **capacidad local** de las CNN para capturar texturas y bordes.
* La **capacidad global** de los Transformers para modelar relaciones espaciales complejas entre regiones lejanas.

Esto lo hace ideal para tareas donde los patrones pueden estar distribuidos espacialmente, como clasificaciÃ³n, segmentaciÃ³n o detecciÃ³n.

---

## ğŸ“ Estructura de Carpetas

```
model/
â”‚  â”œâ”€â”€ cnn.hpp
â”‚  â”œâ”€â”€ mlp.hpp
â”‚  â”œâ”€â”€ vit_cnn.hpp  â† clase principal VTCNN
â”‚
layers/
â”‚  â”œâ”€â”€ conv2d_layer.hpp
â”‚  â”œâ”€â”€ batch_normalization_layer.hpp
â”‚  â”œâ”€â”€ visual_transformer.hpp
â”‚  â”œâ”€â”€ ...
â”‚
utils/
â”‚  â”œâ”€â”€ load_dataset.hpp
â”‚  â”œâ”€â”€ trainer.hpp
â”‚
main.cpp
```

---

## ğŸ§© Extensiones futuras

* Reemplazar `FilterTokenizer` con `RecurrentTokenizer`.
* AÃ±adir posiciÃ³n codificada (positional encoding) a los tokens.
* Usar `Adam` como optimizador.
* Extender a otras tareas como segmentaciÃ³n o VQA.

---

## ğŸ–¼ï¸ Ejemplo Visual (flujo de datos)

```
Image (28x28x1)
  â†“ Conv2D (1â†’4)  â†’  (28x28x4)
  â†“ BatchNorm
  â†“ Flatten to Sequence (784x4)
  â†“ Visual Transformer (â†’ token 16x16)
  â†“ Projector â†’ Restore (28x28x8)
  â†“ MaxPool (14x14x8)
  â†“ Flatten
  â†“ Dense (â†’ 10)
  â†“ Softmax â†’ Prediction
```

---

## âœï¸ Autor

Desarrollado por \[Tu Nombre]
ImplementaciÃ³n en C++ usando clases propias de `Tensor`, `Layer` y `Trainer`.

---
